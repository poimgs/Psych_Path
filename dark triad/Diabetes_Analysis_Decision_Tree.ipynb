{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "### Our Goal:\n",
    "To diagnostically predict whether a patient has diabetes.\n",
    "\n",
    "### To Explore:\n",
    "1. DecisionTreeClassifier\n",
    "2. BaggingClassifier\n",
    "3. AdaBoostClassifier\n",
    "4. RandomForestClassifier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General libraries needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#For Decision Tree implementation\n",
    "from scipy.stats import entropy\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "\n",
    "#For Bagging implementation\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#For AdaBoost implementation\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split \n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>glucose</th>\n",
       "      <th>bp</th>\n",
       "      <th>skin</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnant  glucose  bp  skin  insulin   bmi  pedigree  age  label\n",
       "0         6      148  72    35        0  33.6     0.627   50      1\n",
       "1         1       85  66    29        0  26.6     0.351   31      0\n",
       "2         8      183  64     0        0  23.3     0.672   32      1\n",
       "3         1       89  66    23       94  28.1     0.167   21      0\n",
       "4         0      137  40    35      168  43.1     2.288   33      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "# load dataset\n",
    "df = pd.read_csv(\"Diabetes.csv\", header = None, names=col_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "#label is the result we trying to predict [1 = got diabetes, 0 = healthy]\n",
    "\n",
    "feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n",
    "X = df[feature_cols]          # Features, independent var\n",
    "y = df.label                  # Target variable, dependent var (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "# Random partitions\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7316017316017316\n"
     ]
    }
   ],
   "source": [
    "#Create tree\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the full tree\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz\n",
    "tree.export_graphviz(clf, out_file='tree.dot', feature_names=feature_cols) #produces dot file\n",
    "\n",
    "import pydot\n",
    "\n",
    "(graph,) = pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('full_tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7186147186147186\n"
     ]
    }
   ],
   "source": [
    "#Pruning! basically stop the tree before it max. \n",
    "#Directly implement the DecisionTreeClassifier on the training set. To ensure pruning, we set the max_depth=4.\n",
    "#Prevents overfitting.. but how do we do know when is too much????\n",
    "\n",
    "dptree = DecisionTreeClassifier(max_depth=4)\n",
    "dptree.fit(X_train, y_train)\n",
    "\n",
    "#Test!\n",
    "# Get the predicted y array\n",
    "\n",
    "y_pred = dptree.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth =  1 Average Accuracy: 0.7197402597402598\n",
      "Depth =  2 Average Accuracy: 0.73987012987013\n",
      "Depth =  3 Average Accuracy: 0.7386147186147184\n",
      "Depth =  4 Average Accuracy: 0.7290043290043289\n",
      "Depth =  5 Average Accuracy: 0.7370995670995673\n",
      "Depth =  6 Average Accuracy: 0.7258874458874456\n",
      "Depth =  7 Average Accuracy: 0.715108225108225\n",
      "Depth =  8 Average Accuracy: 0.7084848484848486\n",
      "Depth =  9 Average Accuracy: 0.7123809523809522\n",
      "Depth =  10 Average Accuracy: 0.7065367965367968\n",
      "Depth =  11 Average Accuracy: 0.7020779220779222\n",
      "Depth =  12 Average Accuracy: 0.7004761904761905\n",
      "Depth =  13 Average Accuracy: 0.7006493506493505\n",
      "Depth =  14 Average Accuracy: 0.7002597402597401\n",
      "Depth =  15 Average Accuracy: 0.7054978354978357\n",
      "Depth =  16 Average Accuracy: 0.7051948051948052\n",
      "Depth =  17 Average Accuracy: 0.7040259740259743\n",
      "Depth =  18 Average Accuracy: 0.6992640692640693\n",
      "Depth =  19 Average Accuracy: 0.697402597402597\n"
     ]
    }
   ],
   "source": [
    "# Create Decision Tree classifer object\n",
    "# possible parameters to pass in and default values:\n",
    "# SEE: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "# criterion = \"gini\" or \"entropy\"\n",
    "# max_depth: int (default, none -- go as deep)\n",
    "# min_samples_split: int (default 2), if use float, it will consider the value as a proportion -- relative to dataset I believe)\n",
    "\n",
    "for depth in range(1, 20):\n",
    "    \n",
    "    pred_sum = 0\n",
    "    \n",
    "    trials = 100\n",
    "    \n",
    "    for trial in range(0, trials):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3) # 70% training and 30% test\n",
    "        \n",
    "        decision_tree_test = DecisionTreeClassifier(\n",
    "            max_depth = depth,\n",
    "        )\n",
    "\n",
    "        dt = decision_tree_test.fit(X_train,y_train)\n",
    "\n",
    "        dt_pred = dt.predict(X_test)\n",
    "        pred_sum += metrics.accuracy_score(y_test, dt_pred)\n",
    "    \n",
    "    print(\"Depth = \", depth , \"Average Accuracy:\", pred_sum/trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Bagging (with Decision Tree) </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[127  32]\n",
      " [ 22  50]]\n"
     ]
    }
   ],
   "source": [
    "#Create the Bagging classifier. Default base classifiers is Decision Tree. \n",
    "# - n_estimator is the number of base classifiers (number of trees trained) (i.e. weak learners)\n",
    "\n",
    "model = BaggingClassifier(n_estimators=200)\n",
    "\n",
    "#Fit the training feature Xs and training label Ys\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix of the result\n",
    "cm = metrics.confusion_matrix(y_pred, y_test)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7662337662337663\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy of the result\n",
    "\n",
    "bagged_results = metrics.accuracy_score(y_pred, y_test)\n",
    "print(bagged_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>AdaBoost (with Decision Tree) </h1>\n",
    "\n",
    "*Note that the default AdaBoost implementation in SKLearn is Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=0.1, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the AdaBoost classifier. Default base classifiers is Decision Tree. \n",
    "# - n_estimator is the number of base classifiers (i.e. weak learners)\n",
    "# - learning_rate controls the weight adjustments of each base classifiers. Default is 1\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=200, learning_rate = 0.1)  #if you change learning_rates/ tune the no of weak base classifier, the \n",
    "#final accuracy will change\n",
    "\n",
    "#Fit the training feature Xs and training label Ys\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#SVC classifer takes long time to run BUT it actually gives a very high accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[123  41]\n",
      " [ 26  41]]\n",
      "0.70995670995671\n"
     ]
    }
   ],
   "source": [
    "#Use the trained model to predict the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix of the result\n",
    "cm = metrics.confusion_matrix(y_pred, y_test)\n",
    "print(cm)\n",
    "\n",
    "# Find the accuracy of the result\n",
    "asr = metrics.accuracy_score(y_pred, y_test)\n",
    "print(asr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learningrate =  0.3\n",
      "Average Accuracy: 0.7545454545454545\n",
      "-----------------------------------\n",
      "learningrate =  0.09\n",
      "Average Accuracy: 0.7642857142857143\n",
      "-----------------------------------\n",
      "learningrate =  0.026999999999999996\n",
      "Average Accuracy: 0.7595238095238095\n",
      "-----------------------------------\n",
      "learningrate =  0.0081\n",
      "Average Accuracy: 0.7467532467532466\n",
      "-----------------------------------\n",
      "learningrate =  0.0024299999999999994\n",
      "Average Accuracy: 0.7361471861471862\n",
      "-----------------------------------\n",
      "learningrate =  0.0007289999999999998\n",
      "Average Accuracy: 0.7277056277056275\n",
      "-----------------------------------\n",
      "learningrate =  0.00021869999999999995\n",
      "Average Accuracy: 0.7183982683982684\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#The bigger the learning rate the more emphathsis on older trees\n",
    "\n",
    "for learningrate in range(1, 8):\n",
    "    \n",
    "    pred_sum = 0\n",
    "    \n",
    "    trials = 20\n",
    "    \n",
    "    for trial in range(0, trials):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3) # 70% training and 30% test\n",
    "        \n",
    "        decision_tree_test = AdaBoostClassifier(n_estimators=200, learning_rate = 0.3 ** learningrate)\n",
    "\n",
    "        dt = decision_tree_test.fit(X_train,y_train)\n",
    "\n",
    "        dt_pred = dt.predict(X_test)\n",
    "        \n",
    "        pred_sum += metrics.accuracy_score(y_test, dt_pred)\n",
    "    \n",
    "    print(\"learningrate = \", 0.3 ** learningrate)\n",
    "    print(\"Average Accuracy:\", pred_sum/trials)\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1> Random Forest </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7316017316017316\n"
     ]
    }
   ],
   "source": [
    "#Create the Random Forest classifier.\n",
    "#n_estimator is the number of base classifiers (i.e. weak learners) number of trees you have\n",
    "#default is sqrt of the m var\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "#SVC classifer takes long time to run BUT it actually gives a very high accuracy\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for n_estimators: 1  is  0.7272727272727273\n",
      "Accuracy for n_estimators: 5  is  0.6926406926406926\n",
      "Accuracy for n_estimators: 25  is  0.7402597402597403\n",
      "Accuracy for n_estimators: 125  is  0.7229437229437229\n",
      "Accuracy for n_estimators: 625  is  0.7445887445887446\n",
      "Accuracy for n_estimators: 3125  is  0.7402597402597403\n",
      "Accuracy for n_estimators: 15625  is  0.7359307359307359\n"
     ]
    }
   ],
   "source": [
    "#put a trail loop inside \n",
    "for power in range(7):\n",
    "    \n",
    "    n_estimators = 5 ** power\n",
    "\n",
    "    random_forest_test = RandomForestClassifier(\n",
    "        n_estimators = n_estimators,\n",
    "        bootstrap = True,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    random_forest_test.fit(X_train, y_train)\n",
    "    y_pred = random_forest_test.predict(X_test)\n",
    "    print(\"Accuracy for n_estimators:\", n_estimators, \" is \", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from your Seniors DA (The Pioneer Batch) : **Ding Yang, Linus Cheng, Tan Kin Meng, and Kaelyn** for these codes, which they used in their DAP sharing :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
